{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "tags = []\n",
    "docs = []\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "#train_ds = 'dataset/ng20/20ng-train-no-stop.txt'\n",
    "train_ds = 'dataset/raw/20ng-train-stemmed.txt'\n",
    "with open(train_ds) as raw_text:\n",
    "    for idx, line in enumerate(raw_text):\n",
    "        tokens = line.strip().split()\n",
    "        tags.append(tokens[0])\n",
    "        docs.append(' '.join(tokens[1:]))\n",
    "        \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf = CountVectorizer(min_df=3,\n",
    "                     max_df=0.80, \n",
    "                     max_features=vocab_size)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(min_df=3,\n",
    "                        max_df=0.80, \n",
    "                        max_features=vocab_size,\n",
    "                        use_idf=True, sublinear_tf=True,\n",
    "                        norm='l2');\n",
    "\n",
    "scheme = 'tfidf'\n",
    "transformer = tfidf\n",
    "\n",
    "x_train = transformer.fit_transform(docs)\n",
    "categories = (list(set(tags)))\n",
    "\n",
    "indices = []\n",
    "for t in tags:\n",
    "    hasMatch = False\n",
    "    for idx, cate in enumerate(categories):\n",
    "        if t == cate:\n",
    "            indices.append(idx)\n",
    "            hasMatch = True\n",
    "            break\n",
    "    \n",
    "    assert(hasMatch)\n",
    "\n",
    "y_train = np.array(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape=(11293, 10000)\n",
      "gnd train shape=(11293, 20)\n",
      "test shape=(7528, 10000)\n",
      "gnd test shape=(7528, 20)\n"
     ]
    }
   ],
   "source": [
    "tags = []\n",
    "docs = []\n",
    "\n",
    "#test_ds = 'dataset/ng20/20ng-test-no-stop.txt'\n",
    "test_ds = 'dataset/raw/20ng-test-stemmed.txt'\n",
    "\n",
    "with open(test_ds) as raw_text:\n",
    "    for idx, line in enumerate(raw_text):\n",
    "        tokens = line.strip().split()\n",
    "        tags.append(tokens[0])\n",
    "        docs.append(' '.join(tokens[1:]))\n",
    "        \n",
    "x_test = transformer.transform(docs)\n",
    "\n",
    "indices = []\n",
    "for t in tags:\n",
    "    hasMatch = False\n",
    "    for idx, cate in enumerate(categories):\n",
    "        if t == cate:\n",
    "            indices.append(idx)\n",
    "            hasMatch = True\n",
    "            break\n",
    "    assert(hasMatch)\n",
    "y_test = np.array(indices)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "gnd_train = enc.fit_transform(y_train.reshape((-1,1)))  \n",
    "gnd_test = enc.transform(y_test.reshape((-1,1)))\n",
    "\n",
    "print(\"train shape={}\".format(x_train.shape))\n",
    "print(\"gnd train shape={}\".format(gnd_train.shape))\n",
    "print(\"test shape={}\".format(x_test.shape))\n",
    "print(\"gnd test shape={}\".format(gnd_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sort by doc length\n",
    "bin_mat = x_train.toarray() > 0\n",
    "X_len = np.sum(bin_mat, axis=1)\n",
    "    \n",
    "indices = np.argsort(X_len)\n",
    "x_train_nz = x_train[indices, :]\n",
    "\n",
    "y_train_nz = gnd_train[indices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove too short document\n",
    "bin_mat = x_train_nz.toarray() > 0\n",
    "X_len = np.sum(bin_mat, axis=1)\n",
    "x_train_nz = x_train_nz[X_len >= 10,:]\n",
    "y_train_nz = y_train_nz[X_len >= 10,:]\n",
    "filtered_indices = indices[X_len >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove too long document\n",
    "bin_mat = x_train_nz.toarray() > 0\n",
    "X_len = np.sum(bin_mat, axis=1)\n",
    "x_train_nz = x_train_nz[X_len <= 500,:]\n",
    "y_train_nz = y_train_nz[X_len <= 500,:]\n",
    "filtered_indices2 = filtered_indices[X_len <= 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reverse_idx = {}\n",
    "for idx, val in enumerate(filtered_indices2):\n",
    "    reverse_idx[val] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_nz = np.asarray(gnd_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_test_nz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after remove empty documents...\n",
      "num train:11016 num tag: 11016 num features:10000\n",
      "num test:7335 num tag: 7335 num features:10000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# # Sort by doc length\n",
    "# bin_mat = x_train.toarray() > 0\n",
    "# X_len = np.sum(bin_mat, axis=1)\n",
    "# indices = np.argsort(X_len)\n",
    "# x_train_nz = x_train[indices, :]\n",
    "# y_train_nz = gnd_train[indices,:]\n",
    "\n",
    "# # Remove too short document\n",
    "# bin_mat = x_train_nz.toarray() > 0\n",
    "# X_len = np.sum(bin_mat, axis=1)\n",
    "# x_train_nz = x_train_nz[X_len >= 10,:]\n",
    "# y_train_nz = y_train_nz[X_len >= 10,:]\n",
    "\n",
    "# # Remove too long document\n",
    "# bin_mat = x_train_nz.toarray() > 0\n",
    "# X_len = np.sum(bin_mat, axis=1)\n",
    "# x_train_nz = x_train_nz[X_len <= 500,:]\n",
    "# y_train_nz = y_train_nz[X_len <= 500,:]\n",
    "\n",
    "# Sort by doc length\n",
    "bin_mat = x_test.toarray() > 0\n",
    "    \n",
    "X_len = np.sum(bin_mat, axis=1)\n",
    "indices = np.argsort(X_len)\n",
    "x_test_nz = x_test[indices,:]\n",
    "\n",
    "y_test_nz = gnd_test[indices, :]\n",
    "    \n",
    "# Remove too short document\n",
    "bin_mat = x_test_nz.toarray() > 0\n",
    "    \n",
    "X_len = np.sum(bin_mat, axis=1)\n",
    "x_test_nz = x_test_nz[X_len >= 10,:]\n",
    "y_test_nz = y_test_nz[X_len >= 10,:]\n",
    "\n",
    "# Remove too long document\n",
    "bin_mat = x_test_nz.toarray() > 0\n",
    "    \n",
    "X_len = np.sum(bin_mat, axis=1)\n",
    "x_test_nz = x_test_nz[X_len <= 500,:]\n",
    "y_test_nz = y_test_nz[X_len <= 500,:]\n",
    "\n",
    "print('after remove empty documents...')\n",
    "print('num train:{} num tag: {} num features:{}'.format(x_train_nz.shape[0], \n",
    "                                                        y_train_nz.shape[0], \n",
    "                                                        x_train_nz.shape[1]))\n",
    "print('num test:{} num tag: {} num features:{}'.format(x_test_nz.shape[0], \n",
    "                                                       y_test_nz.shape[0], \n",
    "                                                       x_test_nz.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11016, 10000)\n",
      "(11016, 20)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_nz.shape)\n",
    "print(y_train_nz.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "dataset = 'ng20'\n",
    "scheme = 'tfidf'\n",
    "\n",
    "scipy.io.savemat('dataset/{}/{}.{}.mat'.format(dataset, dataset, scheme), \n",
    "                 mdict={'train': x_train_nz, 'test': x_test_nz, \n",
    "                        'gnd_train': y_train_nz.toarray(), 'gnd_test': y_test_nz.toarray()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save vocabs\n",
    "import pickle\n",
    "pickle.dump(transformer.vocabulary_, open(\"dataset/{}/{}.{}.vocabs.p\".format(dataset, dataset, scheme), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create CV and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Check if 2 dataset are the same\n",
    "tfidf_ds = scipy.io.loadmat('dataset/{}/{}.tfidf.mat'.format(dataset, dataset))\n",
    "#tf_ds = scipy.io.loadmat('dataset/{}/{}.tf.mat'.format(dataset, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gnd_test = tfidf_ds['gnd_test']\n",
    "gnd_train = tfidf_ds['gnd_train']\n",
    "\n",
    "# compute the label distribution\n",
    "label_counts = np.sum(gnd_test, axis=0)\n",
    "label_distrib = label_counts / np.sum(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013775955152313761\n",
      "0.0013671041113509923\n"
     ]
    }
   ],
   "source": [
    "n_samples = gnd_test.shape[0]\n",
    "indices = np.random.permutation(n_samples)\n",
    "\n",
    "n_cv = int(n_samples / 2.)\n",
    "n_test = n_samples - n_cv\n",
    "\n",
    "y_cv = gnd_test[indices[:n_cv],:]\n",
    "y_cv_counts = np.sum(y_cv, axis=0)\n",
    "y_cv_distrib = y_cv_counts / np.sum(y_cv_counts)\n",
    "\n",
    "y_test = gnd_test[indices[n_cv:],:]\n",
    "y_test_counts = np.sum(y_test, axis=0)\n",
    "y_test_distrib = y_test_counts / np.sum(y_test_counts)\n",
    "\n",
    "# compute KL Divergence from CV to train\n",
    "cv_kl = np.sum(np.multiply(label_distrib, (np.log(label_distrib) - np.log(y_cv_distrib))))\n",
    "test_kl = np.sum(np.multiply(label_distrib, (np.log(label_distrib) - np.log(y_test_distrib))))\n",
    "\n",
    "print(cv_kl)\n",
    "print(test_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_tf_cv = tf_ds['test'][indices[:n_cv],:]\n",
    "# x_tf_test = tf_ds['test'][indices[n_cv:],:]\n",
    "# y_tf_cv = tf_ds['gnd_test'][indices[:n_cv],:]\n",
    "# y_tf_test = tf_ds['gnd_test'][indices[n_cv:],:]\n",
    "\n",
    "x_tfidf_cv = tfidf_ds['test'][indices[:n_cv],:]\n",
    "x_tfidf_test = tfidf_ds['test'][indices[n_cv:],:]\n",
    "y_tfidf_cv = tfidf_ds['gnd_test'][indices[:n_cv],:]\n",
    "y_tfidf_test = tfidf_ds['gnd_test'][indices[n_cv:],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(x_tf_cv.shape)\n",
    "print(x_tf_test.shape)\n",
    "print(y_tf_cv.shape)\n",
    "print(y_tf_test.shape)\n",
    "print(x_tfidf_cv.shape)\n",
    "print(x_tfidf_test.shape)\n",
    "print(y_tfidf_cv.shape)\n",
    "print(y_tfidf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_tf_train = tf_ds['train']\n",
    "y_tf_train = tf_ds['gnd_train']\n",
    "\n",
    "scipy.io.savemat('dataset/{}/{}.tf.mat'.format(dataset, dataset), \n",
    "                                              mdict={ 'train': x_tf_train, \n",
    "                                                      'test': x_tf_test, \n",
    "                                                      'cv': x_tf_cv,\n",
    "                                                      'gnd_train': y_tf_train, \n",
    "                                                      'gnd_test': y_tf_test,\n",
    "                                                      'gnd_cv': y_tf_cv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_tfidf_train = tfidf_ds['train']\n",
    "y_tfidf_train = tfidf_ds['gnd_train']\n",
    "\n",
    "scipy.io.savemat('dataset/{}/{}.tfidf.mat'.format(dataset, dataset), \n",
    "                                                 mdict={ 'train': x_tfidf_train, \n",
    "                                                      'test': x_tfidf_test, \n",
    "                                                      'cv': x_tfidf_cv,\n",
    "                                                      'gnd_train': y_tfidf_train, \n",
    "                                                      'gnd_test': y_tfidf_test,\n",
    "                                                      'gnd_cv': y_tfidf_cv})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save indices\n",
    "scipy.io.savemat('dataset/{}/{}.indices.mat'.format(dataset, dataset), mdict={'indices': indices})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a binary dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "\n",
    "# Check if 2 dataset are the same\n",
    "tf_ds = scipy.io.loadmat('dataset/{}/{}.tf.mat'.format(dataset, dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = tf_ds['train']\n",
    "x_test = tf_ds['test']\n",
    "x_cv = tf_ds['cv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_b_train = x_train > 0\n",
    "x_b_test = x_test > 0\n",
    "x_b_cv = x_cv > 0\n",
    "\n",
    "x_b_train = x_b_train.astype(float)\n",
    "x_b_test = x_b_test.astype(float)\n",
    "x_b_cv = x_b_cv.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = tf_ds['gnd_train']\n",
    "y_test = tf_ds['gnd_test']\n",
    "y_cv = tf_ds['gnd_train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.io.savemat('dataset/{}/{}.b.mat'.format(dataset, dataset), mdict={ 'train': x_b_train, \n",
    "                                                      'test': x_b_test, \n",
    "                                                      'cv': x_b_cv,\n",
    "                                                      'gnd_train': y_tfidf_train, \n",
    "                                                      'gnd_test': y_tfidf_test,\n",
    "                                                      'gnd_cv': y_tfidf_cv})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research2018",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

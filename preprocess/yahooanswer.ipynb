{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "dataset = 'yahooanswer'\n",
    "\n",
    "home = str(Path.home())\n",
    "root_dir = os.path.join(home, 'datasets', dataset)\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "def create_dataframe(doc_tf, doc_targets):\n",
    "    docs = []\n",
    "    for i, bow in enumerate(doc_tf):\n",
    "        d = {'doc_id': i, 'bow': bow, 'label': doc_targets[i]}\n",
    "        docs.append(d)\n",
    "    return pd.DataFrame.from_dict(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = os.path.join(root_dir, 'train.csv')\n",
    "df = pd.read_csv(train_fn, header=None)\n",
    "df.columns = ['label', 'title', 'body', 'answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = list(df.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = os.path.join(root_dir, 'train.csv')\n",
    "df = pd.read_csv(train_fn, header=None)\n",
    "df.columns = ['label', 'title', 'body', 'answer']\n",
    "train_content = list(df.title)\n",
    "train_label = list(df.label - 1)\n",
    "\n",
    "test_fn = os.path.join(root_dir, 'test.csv')\n",
    "df = pd.read_csv(test_fn, header=None)\n",
    "df.columns = ['label', 'title', 'body', 'answer']\n",
    "test_content = list(df.title)\n",
    "test_label = list(df.label - 1)\n",
    "\n",
    "################################################################################################\n",
    "count_vect = CountVectorizer(stop_words='english', max_features=12000, max_df=0.8, min_df=3)\n",
    "train_tf = count_vect.fit_transform(train_content)\n",
    "test_tf = count_vect.transform(test_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_dataframe(train_tf, train_label)\n",
    "test_df = create_dataframe(test_tf, test_label)\n",
    "\n",
    "def get_doc_length(doc_bow):\n",
    "    return doc_bow.sum()\n",
    "\n",
    "# remove an empty document\n",
    "train_df = train_df[train_df.bow.apply(get_doc_length) > 0]\n",
    "test_df = test_df[test_df.bow.apply(get_doc_length) > 0]\n",
    "\n",
    "# split test and cv\n",
    "num_train = len(train_df)\n",
    "num_test = len(test_df) // 2\n",
    "num_cv = len(test_df) - num_test\n",
    "\n",
    "test_df = shuffle(test_df)\n",
    "cv_df = test_df.iloc[:num_cv]\n",
    "test_df = test_df.iloc[num_cv:]\n",
    "\n",
    "# set doc_id as an index\n",
    "train_df.set_index('doc_id', inplace=True)\n",
    "test_df.set_index('doc_id', inplace=True)\n",
    "cv_df.set_index('doc_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample the dataset\n",
    "train_df = train_df.sample(n=100000, replace=False)\n",
    "test_df = test_df.sample(n=10000, replace=False)\n",
    "cv_df = cv_df.sample(n=10000, replace=False)\n",
    "\n",
    "# save the dataframes\n",
    "train_df.to_pickle('../dataset/{}/train.tf.df.pkl'.format(dataset))\n",
    "test_df.to_pickle('../dataset/{}/test.tf.df.pkl'.format(dataset))\n",
    "cv_df.to_pickle('../dataset/{}/cv.tf.df.pkl'.format(dataset))\n",
    "\n",
    "# save vocab\n",
    "with open('../dataset/{}/vocab.pkl'.format(dataset), 'wb') as handle:\n",
    "    pickle.dump(count_vect.vocabulary_, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF format\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "# compute DocFrequency\n",
    "doc_freq = np.zeros(train_df.iloc[0].bow.shape[1])\n",
    "for index, row in train_df.iterrows():\n",
    "    bow = (row.bow.toarray().squeeze() > 0).astype(np.float)\n",
    "    doc_freq += bow\n",
    "    \n",
    "# compute inverse document frequency\n",
    "N = len(train_df)\n",
    "idf = np.log(1. + N / doc_freq)\n",
    "\n",
    "def create_tfidf_matrix(doc_tf_df, docidf):\n",
    "    # create TFIDF\n",
    "    doc_tfidf = []\n",
    "    for index, row in doc_tf_df.iterrows():\n",
    "        bow = (row.bow.toarray().squeeze()).astype(np.float)\n",
    "        tfidf = csr_matrix(np.log1p(bow) * docidf)\n",
    "        doc_tfidf.append(tfidf)\n",
    "    return vstack(doc_tfidf)\n",
    "\n",
    "train_tfidf_df = create_dataframe(create_tfidf_matrix(train_df, idf), list(train_df.label))\n",
    "test_tfidf_df = create_dataframe(create_tfidf_matrix(test_df, idf), list(test_df.label))\n",
    "cv_tfidf_df = create_dataframe(create_tfidf_matrix(cv_df, idf), list(cv_df.label))\n",
    "\n",
    "# save the dataframes\n",
    "train_tfidf_df.set_index('doc_id', inplace=True)\n",
    "test_tfidf_df.set_index('doc_id', inplace=True)\n",
    "cv_tfidf_df.set_index('doc_id', inplace=True)\n",
    "\n",
    "train_tfidf_df.to_pickle('../dataset/{}/train.tfidf.df.pkl'.format(dataset))\n",
    "test_tfidf_df.to_pickle('../dataset/{}/test.tfidf.df.pkl'.format(dataset))\n",
    "cv_tfidf_df.to_pickle('../dataset/{}/cv.tfidf.df.pkl'.format(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary format\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "\n",
    "def create_bin_matrix(doc_tf_df):\n",
    "    # create TFIDF\n",
    "    doc_bin = []\n",
    "    for index, row in doc_tf_df.iterrows():\n",
    "        bow = (row.bow.toarray().squeeze() > 0).astype(np.float)\n",
    "        bow = csr_matrix(bow)\n",
    "        doc_bin.append(bow)\n",
    "    return vstack(doc_bin)\n",
    "\n",
    "train_bin_df = create_dataframe(create_bin_matrix(train_df), list(train_df.label))\n",
    "test_bin_df = create_dataframe(create_bin_matrix(test_df), list(test_df.label))\n",
    "cv_bin_df = create_dataframe(create_bin_matrix(cv_df), list(cv_df.label))\n",
    "\n",
    "# save the dataframes\n",
    "train_bin_df.to_pickle('../dataset/{}/train.bin.df.pkl'.format(dataset))\n",
    "test_bin_df.to_pickle('../dataset/{}/test.bin.df.pkl'.format(dataset))\n",
    "cv_bin_df.to_pickle('../dataset/{}/cv.bin.df.pkl'.format(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research2018",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
